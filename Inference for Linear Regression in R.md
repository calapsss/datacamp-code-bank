
## Inferential Ideas

### Regression Output Example I
The following code provides two equivalent methods for calculating the most important pieces of the linear model output. Recall that the p-value is the probability of the observed data (or more extreme) given the null hypothesis is true. As with inference in other settings, you will need the sampling distribution for the statistic (here the slope) assuming the null hypothesis is true. You will generate the null sampling distribution in later chapters, but for now, assume that the null sampling distribution is correct. Additionally, notice that the standard error of the slope and intercept estimates describe the variability of those estimates.
```{r}
# Load the mosaicData package and the RailTrail data

library(mosaicData)

data(RailTrail)

  

# Fit a linear model

ride_lm <- lm(volume ~ hightemp, data = RailTrail)

  

# View the summary of your model

summary(ride_lm)

  

# Print the tidy model output

tidy(ride_lm)
```
### First random sample, second random sample

Now, you will dive in to understanding how linear models vary from sample to sample. Here two random samples from a population are plotted onto the scatterplot. The population data (called `popdata`) already exists and is pre-loaded, along with `ggplot` and `dplyr`.

**1/3**
```{r}

# Using popdata, plot response vs. explanatory

ggplot(popdata, aes(x = explanatory, y = response)) +

# Add a point layer

geom_point() +

# Add a smooth trend layer, using lin. reg., no ribbon

geom_smooth(method = "lm", se = FALSE)
```

**2/3**

```{r}
# Set the random number generator seed for reproducibility

set.seed(4747)

  

# From popdata, randomly sample 50 rows without replacement

sample1 <- popdata %>%

sample_n(size = 50)

  

# Do the same again

sample2 <- popdata %>%

sample_n(size = 50)

  

# Combine both samples

both_samples <- bind_rows(sample1, sample2, .id = "replicate")

  

# See the result

glimpse(both_samples)
```

**3/3**
```{r}
# From previous step

set.seed(4747)

both_samples <- bind_rows(

popdata %>% sample_n(size = 50),

popdata %>% sample_n(size = 50),

.id = "replicate"

)

  

# Using both_samples, plot response vs. explanatory, colored by replicate

ggplot(both_samples,aes(x= explanatory, y= response, color = replicate)) +

# Add a point layer

geom_point() +

# Add a smooth trend layer, using lin. reg., no ribbon

geom_smooth(method = "lm", se = FALSE)
```

### Superimpose lines

Building on the previous exercise, you will now repeat the sampling process 100 times in order to visualize the sampling distribution of regression lines generated by 100 different random samples of the population.

Rather than repeatedly calling `sample_n()`, like you did in the previous exercise, `rep_sample_n()` from the `oilabs` package provides a convenient way to generate many random samples. The function `rep_sample_n()` repeats the `sample_n()` command `reps` times.

The function `do()` from `dplyr` will allow you to run the `lm` call separately for each level of a variable that has been `group_by`'ed. Here, the group variable is the sampling replicate, so each `lm` is run on a different random sample of the data.

**1/4**
```r
# Set the seed for reproducibility

set.seed(4747)

  

# Repeatedly sample the population without replacement

many_samples <- popdata %>%

rep_sample_n(size = 50, reps = 100)

  

# See the result

glimpse(many_samples)
```

**2/4**
```r
# From previous step

set.seed(4747)

many_samples <- popdata %>% rep_sample_n(size = 50, reps = 100)

  

# Using many_samples, plot response vs. explanatory, grouped by replicate

ggplot(data = many_samples, aes(x = explanatory, y = response, group = replicate)) +

# Add a point layer

geom_point() +

# Add a smooth trend line, using lin. reg., no ribbon

geom_smooth(method= "lm", se = FALSE)
```

**3/4**
```r
# From previous step

set.seed(4747)

many_samples <- popdata %>% rep_sample_n(size = 50, reps = 100)

  

many_lms <- many_samples %>%

# Group by replicate

group_by(replicate) %>%

# Run the model on each replicate, then tidy it

do(lm(response ~ explanatory, data = .) %>% tidy()) %>%

# Filter for rows where the term is explanatory

filter(term == "explanatory")

  

# See the result

many_lms
```

**4/4**

```r
# From previous steps

set.seed(4747)

many_samples <- popdata %>% rep_sample_n(size = 50, reps = 100)

many_lms <- many_samples %>%

group_by(replicate) %>%

do(lm(response ~ explanatory, data=.) %>% tidy()) %>%

filter(term == "explanatory")

  

# Using many_lms, plot estimate

ggplot(many_lms, aes(x = estimate)) +

# Add a histogram layer

geom_histogram()
```


### Regression Hypothesis
A regression is run to investigate whether additional hours studied (the explanatory variable) is associated with a higher exam score (the response variable). Should the researchers run a test with a one- or two-sided alternative hypothesis?
**one-sided because the researchers are trying to demonstrate a positive association.**

Right! If the researchers were interested in investigating whether exam scores were higher _or lower_, then they should use a two-sided alternative hypothesis.

### Original Population Change Sample Size

In order to understand the sampling distribution associated with the slope coefficient, it is valuable to visualize the impact changes in the sample and population have on the slope coefficient. Here, changing the sample size directly impacts how variable the slope is.

**1/2**
```r
set.seed(4747)

  

# Generate 100 random samples of size 50

many_samples <- popdata %>%

rep_sample_n(size = 50, reps = 100)

  

# Using many_samples, plot response vs. explanatory, grouped by replicate

ggplot(many_samples, aes(x = explanatory, y = response, group = replicate)) +

# Add a point layer

geom_point() +

# Add a smooth trend layer, using lin. reg., no ribbon

geom_smooth(method = "lm", se = FALSE)
```

**2/2**
```r
set.seed(4747)

  

# Edit the code to take samples of size 10

many_samples <- popdata %>%

rep_sample_n(size = 10, reps = 100)

  

# Draw the plot again; how is it different?

ggplot(many_samples, aes(x = explanatory, y = response, group = replicate)) +

geom_point() +

geom_smooth(method = "lm", se = FALSE)
```

### Hypothetical population - less variability around the line

In order to understand the sampling distribution associated with the slope coefficient, it is valuable to visualize the impact changes in the sample and population have on the slope coefficient. Here, reducing the variance associated with the _response_ variable around the line changes the variability associated with the slope statistics.

```r
# Update the sampling to use new_popdata

many_samples <- new_popdata %>%

rep_sample_n(size = 50, reps = 100)

  

# Rerun the plot; how does it change?

ggplot(many_samples, aes(x = explanatory, y = response, group = replicate)) +

geom_point() +

geom_smooth(method = "lm", se = FALSE)
```

### Hypothetical population - less variability in x direction

In order to understand the sampling distribution associated with the slope coefficient, it is valuable to visualize the impact changes in the sample and population have on the slope coefficient. Here, reducing the variance associated with the _explanatory_ variable around the line changes the variability associated with the slope statistics.

```r
# Take 100 samples of size 50

many_samples <- even_newer_popdata %>%

rep_sample_n(size = 50, reps = 100)

  

# Update and rerun the plot; how does it change?

ggplot(many_samples, aes(x = explanatory, y = response, group = replicate)) +

geom_point() +

geom_smooth(method = "lm", se = FALSE) +

# Set the x-axis limit from -17 to 17

xlim(-17,17)
```

### What changes the variability of the coefficients?

The last three exercises have demonstrated how the variability in the slope coefficient can change based on changes to the population and the sample. Which of the following combinations _increases_ the variability in the sampling distribution of the slope coefficient?

**Smaller sample size, larger variability around the line, decreased range of explanatory variable.**



## Simulation-based inference for the slope parameter

### Null sampling distribution of the slope

In the previous chapter, you investigated the sampling distribution of the slope from a population where the slope was non-zero. Typically, however, to do inference, you will need to know the sampling distribution of the slope under the hypothesis that there is no relationship between the explanatory and response variables. Additionally, in most situations, you don't know the population from which the data came, so the null sampling distribution must be derived from only the original dataset.

In the mid-20th century, a study was conducted that tracked down identical twins that were separated at birth: one child was raised in the home of their biological parents and the other in a foster home. In an attempt to answer the question of whether intelligence is the result of nature or nurture, both children were given IQ tests. The resulting data is given for the IQs of the foster twins (`Foster` is the response variable) and the IQs of the biological twins (`Biological` is the explanatory variable).

In this exercise you'll use the `pull()` function. This function takes a data frame and returns a selected column as a vector (similar to `$`).

**1/2**
```r
library(infer)

  

# Calculate the observed slope

# Run a lin. reg. of Foster vs. Biological on the twins data

obs_slope <- lm(Foster ~ Biological, data = twins) %>%

# Tidy the result

tidy() %>%

# Filter for rows where term equal Biological

filter(term == "Biological") %>%

# Pull out the estimate column

pull(estimate)

  

# See the result

obs_slope
```

**2/2**

```r
library(infer)

set.seed(4747)

  

# Simulate 10 slopes with a permuted dataset

perm_slope <- twins %>%

# Specify Foster vs. Biological

specify(Foster ~ Biological) %>%

# Use a null hypothesis of independence

hypothesize(null = "independence") %>%

# Generate 10 permutation replicates

generate(reps = 10, type = "permute") %>%

# Calculate the slope statistic

calculate(stat = "slope")

  

# See the result

perm_slope
```


### SE of the slope

The previous exercise generated 10 different slopes under a model of no (i.e., null) relationship between the explanatory and response variables. Now repeat the null slope calculations 1000 times to derive a null sampling distribution for the slope coefficient. The null sampling distribution will be used as a benchmark against which to compare the original data. Then, you'll calculate the mean and standard deviation of the null sampling distribution for the slope.

**1/3**
```r
# Simulate 500 slopes with a permuted dataset

perm_slope <- twins %>%

# Specify Foster vs. Biological

specify(Foster ~ Biological) %>%

# Use a null hypothesis of independence

hypothesize(null = "independence") %>%

# Generate 500 permutation replicates

generate(reps = 500, type = "permute") %>%

# Calculate the slope statistic

calculate(stat = "slope")
```

**2/3**
```r
# From previous step

perm_slope <- twins %>%

specify(Foster ~ Biological) %>%

hypothesize(null = "independence") %>%

generate(reps = 500, type = "permute") %>%

calculate(stat = "slope")

  

# Using perm_slope, plot stat

ggplot(perm_slope, aes(x = stat)) +

# Add a density layer

geom_density()
```

**3/3**

```r
# From previous step

perm_slope <- twins %>%

specify(Foster ~ Biological) %>%

hypothesize(null = "independence") %>%

generate(reps = 500, type = "permute") %>%

calculate(stat = "slope")

  

perm_slope %>%

# Ungroup the dataset

ungroup() %>%

# Calculate summary statistics

summarize(

# Mean of stat

mean_stat = mean(stat),

# Std error of stat

std_err_stat = sd(stat)

)
```

### p-value

Now that you have created the null sampling distribution, you can use it to find the p-value associated with the original slope statistic from the `twins` data. Although you might first consider this to be a one-sided research question, instead, use the absolute value function for practice performing two-sided tests on a slope coefficient.

You can calculate the proportion of `TRUE` values in a logical vector using `mean()`. For example, given a numeric vector `x`, the proportion of cases where `x` is greater than or equal to `10` can be calculated using `mean(x >= 10)`.

**1/2**

```r
# Run a lin. reg. of Foster vs. Biological on twins

abs_obs_slope <- lm(Foster ~ Biological, data = twins) %>%

# Tidy the result

tidy() %>%

# Filter for rows where term equals Biological

filter(term == "Biological") %>%

# Pull out the estimate

pull(estimate) %>%

# Take the absolute value

abs()
```


**2/2**

```r
# From previous step

abs_obs_slope <- lm(Foster ~ Biological, data = twins) %>%

tidy() %>%

filter(term == "Biological") %>%

pull(estimate) %>%

abs()

  

# Compute the p-value

perm_slope %>%

# Add a column of the absolute value of the slope

mutate(abs_perm_slope = abs(stat)) %>%

# Calculate a summary statistic

summarize(

# Get prop. cases where abs. permuted slope is greater than or equal to abs. observed slope

p_value = mean(abs_perm_slope >= abs_obs_slope)

)
```


### Inference on slope

What can we conclude based on the p-value associated with the twins data?

**If there were no association between foster and biological twin IQ (no nature) in the population, we would be extremely unlikely to have collected a sample of data like we did.**


### Bootstrapping the data

Using the `infer` package with `type = "bootstrap"`, you can repeatedly sample from the dataset to estimate the sampling distribution and standard error of the slope coefficient. Using the sampling distribution will allow you to directly find a confidence interval for the underlying population slope.

```r
# Set the seed for reproducibility

set.seed(4747)

  

# Calculate 1000 bootstrapped slopes

boot_slope <- twins %>%

# Specify Foster vs. Biological

specify(Foster ~ Biological) %>%

# Generate 1000 bootstrap replicates

generate(reps = 1000, type = "bootstrap") %>%

# Calculate the slope statistic

calculate(stat = "slope")

  

# See the result

boot_slope
```

### SE method - bootstrap CI for slope

The twins study was used to weigh in on the question of whether IQ is a result of nature (your genes) or nurture (your environment). If IQ was purely a result of nature, what slope would you expect to see in your linear model?

Recall that one way to create a confidence interval for a statistic is to calculate the statistic repeatedly under different bootstrap samples and to find the standard deviation associated with the bootstrapped statistics.

The `twins` data is already loaded in your workspace.

```r
set.seed(4747)

  

# Calculate the slope statistic

# from 1000 bootstrap replicates of

# the Foster vs. Biological model

# of the twins dataset

boot_slope <- twins %>%

specify(Foster ~ Biological) %>%

generate(reps = 1000, type = "bootstrap") %>%

calculate(stat = "slope")

  

# Create a confidence interval of stat

# 2 std devs each side of the mean

boot_slope %>%

summarize(

lower = mean(stat) - 2 * sd(stat),

upper = mean(stat) + 2 * sd(stat)

)
```


### Percentile method - bootstrap CI for slope

Alternatively, a CI for the slope can be created using the percentiles of the distribution of the bootstrapped slope statistics. Recall that a CI is created in such a way that, over a lifetime of analysis, the coverage rate of a CI is (1-alpha)*100%. If you always set alpha = 0.05, then the 95% confidence intervals will capture the parameter of interest (over your lifetime) 95% of the time. Typically, out of the 5% of the time when the interval misses the parameter, sometimes the interval is too high (2.5% of the time) and sometimes the interval is too low (2.5% of the time).

The bootstrapped estimates of `slope`, `boot_slope`, are loaded in your workspace.

```r
# Set alpha

alpha <- 0.05

  

# Set the lower percentile

p_lower <- alpha / 2

  

# Set the upper percentile

p_upper <- 1 - alpha / 2

  

# Create a confidence interval of stat using quantiles

boot_slope %>%

summarize(

lower = quantile(stat, p_lower),

upper = quantile(stat, p_upper)

)
```

### Inference from randomization and bootstrapped distributions

Throughout this chapter we have investigated the slope associated with the regression of `Foster` twins on `Biological` twins. The inference question was based on a randomization test assuming no relationship between the two types of twins (i.e., a slope of zero). The confidence intervals investigated a research question associated with a 100% nature relationship (i.e., a slope of one). What are the appropriate conclusions of this study?

**Zero is not a plausible value for the population slope, one is a plausible value for the population slope.**





## t-Based Inference For the Slope Parameter
### How do the theoretical results play a role?

Instead of simulating a null distribution (using permutations), the t-distribution can be used to calculate p-values and confidence intervals. The theoretical result provides a t-distribution fit for the sampling distribution of the standardized slope statistic. Why does it matter if the sampling distribution is accurate?

**Some of the Above**


### t-statistic

Using the permuted datasets (recall, the randomization forces the null hypothesis to be true), investigate the distribution of the standardized slope statistics (the slope, which has been divided by the standard error). Note that the distribution of the standardized slope is well described by a t-distribution.

**1/2**

```r
# Look at the data

twins_perm

  

# Filter for Biological_perm

biological_perm <- twins_perm %>%

filter(term == "Biological_perm")

  

# Calculate degrees of freedom of twins

degrees_of_freedom <- nrow(twins) - 2
```

**2/2**

```r
# From previous step

biological_perm <- twins_perm %>%

filter(term == "Biological_perm")

degrees_of_freedom <- nrow(twins) - 2

  

# Using biological_perm, plot statistic

ggplot(biological_perm, aes(x = statistic)) +

# Add a histogram layer, with density on the y axis

geom_histogram(aes(y = ..density..)) +

# Add a t-distribution function stat, colored red

stat_function(fun = dt, args = list(df = degrees_of_freedom), color = "red")
```

### Working with R-output (1)

The p-value given by the `lm` output is a two-sided p-value by default. In the twin study, it might seem more reasonable to follow along the one-sided scientific hypothesis that the IQ scores of the twins are _positively_ associated. Because the p-value is the probability of the observed data or more extreme, the two-sided test p-value is twice as big as the one-sided result. That is, to get a one-sided p-value from the two-sided output in R, divide the p-value by two.

The linear regression model of `Foster` vs. `Biological`, `model`, is provided in the script.

```r
model <- lm(Foster ~ Biological, data = twins)

  

# Get the Biological model coefficient

biological_term <- model %>%

# Tidy the model

tidy() %>%

# Filter for term equal to "Biological"

filter(term == "Biological")

  

biological_term %>%

# Add a column of one-sided p-values

mutate(one_sided_p_value = p.value / 2)
```

### Working with R-output (2)

In thinking about the scientific research question, if IQ is caused only by genetics, then we would expect the slope of the line between the two sets of twins to be 1. Testing the hypothesized slope value of 1 can be done by making a new test statistic which evaluates how far the observed slope is from the hypothesized value of 1.

newt=slope−1SE

If the hypothesis that the slope equals one is true, then the new test statistic will have a t-distribution which we can use for calculating a p-value.

The biological term from the model is available as `biological_term`

```r
# Calculate the degrees of freedom of twins

degrees_of_freedom <- nrow(twins) - 2

  

biological_term %>%

mutate(

# Calculate the test statistic

test_statistic = (estimate - 1) / std.error,

# Calculate its one-sided p-value

one_sided_p_value_of_test_statistic = pt(test_statistic, df = degrees_of_freedom),

# ... and its two-sided p-value

two_sided_p_value_of_test_statistic = 2 * one_sided_p_value_of_test_statistic

)
```

### Comparing randomization inference and t-inference

When technical conditions (see next chapter) hold, the inference from the randomization test and the t-distribution test should give equivalent conclusions. They will not provide the exact same answer because they are based on different methods. But they should give p-values and confidence intervals that are reasonably close.

```r
# The slope in the observed data and each permutation replicate

obs_slope

perm_slope

  

# Calculate the absolute value of the observed slope

abs_obs_slope <- abs(obs_slope)

  

# Find the p-value

perm_slope %>%

# Add a column for the absolute value of stat

mutate(abs_perm_slope = abs(stat)) %>%

summarize(

# Calculate prop'n permuted values at least as extreme as observed

p_value = mean(abs_perm_slope >= abs_obs_slope)

)
```

### CI using t-theory

In previous courses, you have created confidence intervals with the formula of statistic plus/minus some number of standard errors. With bootstrapping, we typically use two standard errors. With t-based theory, we use the specific t-multiplier.

Create a CI for the slope parameter using both the default `tidy()` call as well as `mutate()` to calculate the confidence interval bounds explicitly. Note that the two methods should give exactly the same CI values because they are using the same computations.

`alpha` has been set to `0.05` and the degrees of freedom of the `twins` dataset is given to you.

**1/2**

```r
alpha <- 0.05

degrees_of_freedom <- nrow(twins) - 2

  

# Calculate the confidence level

confidence_level <- 1 - alpha

  

# Calculate the upper percentile cutoff

p_upper <- 1 - alpha / 2

  

# Find the critical value from the t-distribution

critical_value <- qt(p_upper, df = degrees_of_freedom)
```

**2/2**

```r
# From previous step

alpha <- 0.05

degrees_of_freedom <- nrow(twins) - 2

confidence_level <- 1 - alpha

p_upper <- 1 - alpha / 2

critical_value <- qt(p_upper, df = degrees_of_freedom)

  

tidied_model <- lm(Foster ~ Biological, data = twins) %>%

# Tidy the model, with a confidence interval

tidy(conf.int = TRUE, conf.level = confidence_level)

  

tidied_model %>%

# Manually calculate the same interval

mutate(

lower = estimate - critical_value * std.error,

upper = estimate + critical_value * std.error

)
```

### Comparing randomization CIs and t-based CIs

As with hypothesis testing, if technical conditions hold (technical conditions are discussed more in the next chapter), the CI created for the slope parameter in the t-distribution setting should be in line with the CI created using bootstrapping. Create a CI for the slope parameter and compare it to the one created using the bootstrap percentile interval from the previous chapter.

Note that the bootstrap and t-intervals will not be exactly the same because they use different computational steps to arrive at the interval estimate.

**1/2**

```r
alpha <- 0.05

  

# Calculate the confidence level

confidence_level <- 1 - alpha

  

# Calculate lower percentile cutoff

p_lower <- alpha / 2

  

# ... and the upper one

p_upper <- 1 - alpha / 2
```

**2/2**

```r
# From previous step

alpha <- 0.05

confidence_level <- 1 - alpha

p_lower <- alpha / 2

p_upper <- 1 - alpha / 2

  

# Tidy the model, including a confidence interval

tidied_model <- lm(Foster ~ Biological, data = twins) %>%

tidy(conf.int = TRUE, conf.level = confidence_level)

  

# Recall the t-confidence interval

tidied_model %>%

filter(term == "Biological") %>%

select(conf.low, conf.high)

  

# Create the bootstrap confidence interval

boot_slope %>%

summarize(

lower = quantile(stat, p_lower),

upper = quantile(stat, p_upper)

)
```

### Confidence intervals for the average response at specific values

The previous two exercises gave CIs for the slope parameter. That is, based on the observed data, you provided an interval estimate of the plausible values for the true slope parameter in the population. Recall that the number 1 was in the CI for the slope, meaning 1 cannot be ruled out as a possible value for the true slope between Biological and Foster twins. There is no evidence to claim that there is a difference, on average, between the IQ scores of two twins in any given pair.

When working with a linear regression model, you might also want to know the plausible values for the expected value of the response at a particular explanatory location. That is, what would you expect the IQ of a Foster twin to be given a Biological twin's IQ of 100?

**1/3**
```r
model <- lm(Foster ~ Biological, data = twins)

  

# Get observation-level values from the model

augment(model)
```

**2/3**
```r
# From previous step

model <- lm(Foster ~ Biological, data = twins)

  

# Create a dataframe of new observations

new_twins <- data.frame(Biological = seq(70, 130, 15))

  

# Augment the model with the new dataset

augmented_model <- model %>%

augment(newdata = new_twins)

  

# See the result

augmented_model
```

**3/3**
```r
# From previous steps

model <- lm(Foster ~ Biological, data = twins)

new_twins <- data.frame(Biological = seq(70, 130, 15))

augmented_model <- model %>% augment(newdata = new_twins)

  

augmented_model %>%

# Calculate a confidence interval on the predicted values

mutate(

lower_mean_prediction = .fitted - critical_value * .se.fit,

upper_mean_prediction = .fitted + critical_value * .se.fit

)
```


### Confidence intervals for the average response for all observations

The confidence interval for the average response can be computed for all observations in the dataset. Using `augment()` directly on the `twins` dataset gives predictions and standard errors for the Foster twin based on all the Biological observations.

Note that the calculation of the regression line is more stable at the center, so predictions for the extreme values are more variable than predictions in the middle of the range of explanatory IQs.

The foster twin IQ predictions that you calculated last time are provided as `predictions`. These predictions are shown in a plot using `geom_smooth()`.

```r
# This plot is shown

ggplot(twins, aes(x = Biological, y = Foster)) +

geom_point() +

geom_smooth(method = "lm")

  

ggplot() +

# Add a point layer of Foster vs. Biological, using twins

geom_point(aes(x = Biological, y = Foster), data = twins) +

# Add a line layer of .fitted vs Biological, using predictions, colored blue

geom_line(aes(x = Biological, y = .fitted), data = predictions, color = "blue") +

# Add a ribbon layer of lower_mean_prediction to upper_mean_prediction vs Biological,

# using predictions, transparency of 0.2

geom_ribbon(

aes(x = Biological, ymin = lower_mean_prediction, ymax = upper_mean_prediction),

data = predictions, alpha = 0.2

)
```

### Prediction intervals for the individual response

Along with an interval estimate for the expected value of the response, it is often desired to have an interval estimate for the actual individual responses. The formulation for the prediction is the same, but the predicted points are more variable around the line, so the standard error is calculated to be a larger value.

As with the interval around the expected average values, the interval for predicted individual values is smaller in the middle than on the extremes due to the calculation of the regression line being more stable at the center. Note that the intervals for the average responses are much smaller than the intervals for the individual responses.

You have already seen `tidy()`, to pull out coefficient-level information from a model, and `augment()` for observation-level information. `glance()` completes the triumvirate, giving you model-level information.

The linear regression is provided as `model` and the predictions from the previous exercise are given as `predictions`.

**1/3**

```r
twins_sigma <- model %>%

# Get model-level information

glance() %>%

# Pull out sigma

pull(sigma)

  

predictions %>%

# Calculate the std err of the predictions

mutate(std_err_of_predictions = sqrt(twins_sigma ^ 2 + .se.fit ^ 2))
```

**2/3**

```r
# From previous step

twins_sigma <- model %>% glance() %>% pull(sigma)

predictions2 <- predictions %>%

mutate(std_err_of_predictions = sqrt(twins_sigma ^ 2 + .se.fit ^ 2))

  

predictions3 <- predictions2 %>%

# Calculate the prediction intervals

mutate(

lower_response_prediction = .fitted - critical_value * std_err_of_predictions,

upper_response_prediction = .fitted + critical_value * std_err_of_predictions

)

  

# See the result

predictions3
```

**3/3**
```r
# From previous step3

twins_sigma <- model %>% glance() %>% pull(sigma)

predictions3 <- predictions %>%

mutate(

std_err_of_predictions = sqrt(twins_sigma ^ 2 + .se.fit ^ 2),

lower_response_prediction = .fitted - critical_value * std_err_of_predictions,

upper_response_prediction = .fitted + critical_value * std_err_of_predictions

)

  

# Update the plot

ggplot() +

geom_point(aes(x = Biological, y = Foster), data = twins) +

geom_smooth(aes(x = Biological, y = Foster), data = twins, method = "lm") +

# Add a ribbon layer

geom_ribbon(

# ... of lower_response_prediction to upper_response_prediction vs. Biological

aes(x = Biological, ymin = lower_response_prediction, ymax = upper_response_prediction),

# ... using the predictions3 dataset

data = predictions3,

# ... with transparency set to 0.2

alpha = 0.2,

# ... and fill color red

fill = "red"

)
```



## Technical Conditions in linear regression

### Violation of LINE conditions (1)

Which of the linear regression technical conditions are violated in the given figure?

**Linearity**

### Violation of LINE conditions (2)

Which of the linear regression technical conditions are violated in the given figure?

**E_qual variability around the entire line**

### Using residuals (1)

In the next few exercises, you will calculate residuals from a data set that complies with the linear regression technical conditions. For the linear model conditions to hold, the points should be scattered throughout the residual plot with no discernible pattern. Here, the residual plot looks like a scattering of points.

**(1/2)**
```r
# Using hypdata_nice, draw a scatter plot of response vs. explanatory

ggplot(hypdata_nice, aes(x = explanatory, y = response)) +

geom_point()
```

**(2/2)**

```r
# Run a linear regression of response vs. explanatory

model <- lm(response ~ explanatory, data = hypdata_nice)

  

# Augment to get the observation-level information

modeled_observations <- augment(model)

  

# See the result

modeled_observations

  

# Using modeled_observations, draw a scatter plot of .resid vs. .fitted

ggplot(modeled_observations, aes(x = .fitted, y = .resid)) +

geom_point()
```

### Using residuals (2)

Now, you will calculate residuals from a data set that violates the technical conditions. For the linear model conditions to hold, the points should be scattered throughout the residual plot with no discernible pattern. Here the residuals reveal a violation of the technical conditions.

```r
# Using hypdata_poor, draw a scatter plot of response vs. explanatory

ggplot(hypdata_poor, aes(x = explanatory, y = response)) +

geom_point()
```

```r
# Run a linear regression of response vs. explanatory

model <- lm(response ~ explanatory, data = hypdata_poor)

  

# Augment to get the observation-level information

modeled_observations <- augment(model)

  

# See the result

modeled_observations

  

# Using modeled_observations, draw a scatter plot

# of residuals vs. fitted values

ggplot(modeled_observations, aes(x = .fitted, y = .resid)) +

geom_point()
```

### Why do we need the LINE assumptions?

So far, you have implemented two approaches for performing inference assessment to a linear model. The first way is given by the standard R output (`lm`) and is based on the t-distribution. The derivation of the t-distribution is based on the theory (i.e., the LINE conditions).

The second method uses a randomization test which assumes that the observations are exchangeable under the null hypothesis. That is, when the null hypothesis (X is independent of Y) is true, the Y values can be swapped among the X values. The technical conditions in the randomization setting are linear relationship, independent observations, and equal variances. However, the normality assumption is not needed.

What happens if inferences is performed when the technical conditions are violated?

**SOmeof the above**

### Estimation with and without outlier

The data provided in this exercise (`hypdata_outlier`) has an extreme outlier. A plot is shown of the dataset, and a linear regression model of `response` versus `explanatory`. You will remove the outlying point to see how one observation can affect the estimate of the line.

```r
# This plot is shown

p <- ggplot(hypdata_outlier, aes(x = explanatory, y = response)) +

geom_point() +

geom_smooth(method = "lm", se = FALSE)

  

# Filter to remove the outlier

hypdata_no_outlier <- hypdata_outlier %>%

filter(response < 200)

  

p +

# Add another smooth lin .reg. layer, no ribbon,

# hypdata_no_outlier data, colored red

geom_smooth(

method = "lm", se = FALSE,

data = hypdata_no_outlier, color = "red"

)
```

### Inference with and without outlier (t-test)

Not only can one point change the estimated regression line, but it can also change the inferential analysis.

The datasets with and without the outlier are provided as `hypdata_outlier` and `hypdata_no_outlier` respectively.

```r
# Model response vs. explanatory on hypdata_outlier and tidy it

tidy(lm(response ~ explanatory, data = hypdata_outlier))

  

# Do the same on hypdata_no_outlier

tidy(lm(response ~ explanatory, data = hypdata_no_outlier))
```

### Inference with and without outlier (randomization)

Using the randomization test, you can again evaluate the effect of an outlier on the inferential conclusions of a linear model. Run a randomization test on the `hypdata_out` data twice: once with the outlying value and once without it. Note that the extended lines of code communicate clearly the steps of the randomization tests.

```r
# Calculate the p-value with the outlier

perm_slope_out %>%

mutate(abs_perm_slope = abs(stat)) %>%

summarize(p_value = mean(abs_perm_slope >= abs(obs_slope_out)))

  

# Calculate the p-value without the outlier

perm_slope_noout %>%

mutate(abs_perm_slope = abs(stat)) %>%

summarize(p_value = mean(abs_perm_slope >= abs(obs_slope_noout)))
```

### Adjusting for non-linear relationship

The next three examples work with datasets where the underlying data structure violates the linear regression technical conditions. For each example, you will apply a transformation to the data in order to create residual plots that look scattered.

In this first example, it appears as though the variables are not linearly related.

```r
# Run this to see how the model looks

ggplot(hypdata_nonlinear, aes(x = explanatory, y = response)) +

geom_point() +

geom_smooth(method = "lm", se = FALSE)

  

# Model response vs. explanatory

model <- lm(response ~ explanatory, data = hypdata_nonlinear)

  

# Extract observation-level information

modeled_observations <- augment(model)

  

# See the result

modeled_observations

  

# Using modeled_observations, plot residuals vs. fitted values

ggplot(modeled_observations, aes(x = .fitted, y = .resid)) +

# Add a point layer

geom_point() +

# Add horizontal line at y = 0

geom_hline(yintercept = 0)
```

```r
# Run this to see how the model looks

ggplot(hypdata_nonlinear, aes(x = explanatory + explanatory ^ 2, y = response)) +

geom_point() +

geom_smooth(method = "lm", se = FALSE)

  

# Model response vs. explanatory plus explanatory squared

model <- lm(response ~ explanatory + I(explanatory ^ 2), data = hypdata_nonlinear)

  

# Extract observation-level information

modeled_observations <- augment(model)

  

# See the result

modeled_observations

  

# Using modeled_observations, plot residuals vs. fitted values

ggplot(modeled_observations, aes(x = .fitted, y = .resid)) +

# Add a point layer

geom_point() +

# Add horizontal line at y = 0

geom_hline(yintercept = 0)
```

### Adjusting for non-constant errors

In this next example, it appears as though the variance of the `response` variable increases as the `explanatory` variable increases. Note that the fix in this exercise has the effect of changing both the variability as well as modifying the linearity of the relationship.

```r
# Run this to see how the model looks

ggplot(hypdata_nonequalvar, aes(x = explanatory, y = response)) +

geom_point() +

geom_smooth(method = "lm", se = FALSE)

  

# Model response vs. explanatory

model <- lm(response ~ explanatory, data = hypdata_nonequalvar)

  

# Extract observation-level information

modeled_observations <- augment(model)

  

# See the result

modeled_observations

  

# Using modeled_observations, plot residuals vs. fitted values

ggplot(modeled_observations, aes(x = .fitted, y = .resid)) +

# Add a point layer

geom_point() +

# Add horizontal line at y = 0

geom_hline(yintercept = 0)
```

```r
# Run this to see how the model looks

ggplot(hypdata_nonequalvar, aes(x = explanatory, y = log(response))) +

geom_point() +

geom_smooth(method = "lm", se = FALSE)

  

# Model log-response vs. explanatory

model <- lm(log(response) ~ explanatory, data = hypdata_nonequalvar)

  

# Extract observation-level information

modeled_observations <- augment(model)

  

# See the result

modeled_observations

  

# Using modeled_observations, plot residuals vs. fitted values

ggplot(modeled_observations, aes(x = .fitted, y = .resid)) +

# Add a point layer

geom_point() +

# Add horizontal line at y = 0

geom_hline(yintercept = 0)
```

### Adjusting for non-normal errors

In this last example, it appears as though the points are not normally distributed around the regression line. Again, note that the fix in this exercise has the effect of changing both the variability as well as modifying the linearity of the relationship.

```r
# Run this to see how the model looks

ggplot(hypdata_nonnorm, aes(x = explanatory, y = response)) +

geom_point() +

geom_smooth(method = "lm", se = FALSE)

  

# Model response vs. explanatory

model <- lm(response ~ explanatory, data = hypdata_nonnorm)

  

# Extract observation-level information

modeled_observations <- augment(model)

  

# See the result

modeled_observations

  

# Using modeled_observations, plot residuals vs. fitted values

ggplot(modeled_observations, aes(x = .fitted, y = .resid)) +

# Add a point layer

geom_point() +

# Add horizontal line at y = 0

geom_hline(yintercept = 0)
```


```r
# Run this to see how the model looks

ggplot(hypdata_nonnorm, aes(x = explanatory, y = sqrt(response))) +

geom_point() +

geom_smooth(method = "lm", se = FALSE)

  

# Model response vs. explanatory

model <- lm(sqrt(response) ~ explanatory, data = hypdata_nonnorm)

  

# Extract observation-level information

modeled_observations <- augment(model)

  

# See the result

modeled_observations

  

# Using modeled_observations, plot residuals vs. fitted values

ggplot(modeled_observations, aes(x = .fitted, y = .resid)) +

# Add a point layer

geom_point() +

# Add horizontal line at y = 0

geom_hline(yintercept = 0)
```



## Building on Inference in Simple Linear Regression
### Transformed model

As you saw in the previous chapter, transforming the variables can often transform a model from one where the technical conditions are violated to one where the technical conditions hold. When technical conditions hold, you are able to accurately interpret the inferential output. In the two models below, note how the standard errors and p-values change (although in both settings the p-value is significant).

```r
# Create a tidy model

lm(price ~ bed, data = LAhomes) %>% tidy()

  

# Create a tidy model using the log of both variables

lm(log(price) ~ log(bed), data = LAhomes) %>% tidy()
```

### Interpreting transformed coefficients

Transforming variables is a powerful tool to use when running linear regressions. However the parameter estimates must be carefully interpreted in a model with transformed variables.

Consider data collected by Andrew Bray at Reed College on characteristics of LA Homes in 2010. The model is given below, and your task is to provide the appropriate interpretation of the coefficient on `log(sqft)`?

Note: you must be careful to avoid causative interpretations. Additional square footage does not necessarily cause the price of a specific house to go up. The interpretation of the coefficient describes the estimate of the average price of homes at a given square footage.

You will need to run the linear model before answering the question:

`lm(log(price) ~ log(sqft), data = LAhomes) %>% tidy()`

**Each additional 1% of square footage produces an estimate of the average price which is 1.44% higher.**

### LA Homes, multicollinearity (1)

In the next series of exercises, you will investigate how to interpret the sign (positive or negative) of the slope coefficient as well as the significance of the variables (p-value). You will continue to use the log transformed variables so that the technical conditions hold, but you will not be concerned here with the value of the coefficient.

```r
# Output the tidy model

lm(log(price) ~ log(sqft), data = LAhomes) %>% tidy()
```

### LA Homes, multicollinearity (2)

Repeat the previous exercise, but this time regress the log transformed variable `price` on the new variable `bath` which records the number of bathrooms in a home.

```r
# Output the tidy model

lm(log(price) ~ log(bath), data = LAhomes) %>% tidy()
```

### LA Homes, multicollinearity (3)

Now, regress the log transformed variable `price` on the log transformed variables `sqft` AND `bath`. The model is a three dimensional linear regression model where you are predicting `price` as a plane (think of a piece of paper) above the axes including both `sqft` and `bath`.

```r
# Output the tidy model

lm(log(price) ~ log(sqft) + log(bath), data = LAhomes) %>% tidy()
```

### Inference on coefficients

Using the NYC Italian restaurants dataset (compiled by Simon Sheather in _A Modern Approach to Regression with R_), `restNYC`, you will investigate the effect on the significance of the coefficients when there are multiple variables in the model. Recall, the p-value associated with any coefficient is the probability of the observed data given that the particular variable is independent of the response AND given that **all other variables are included in the model**.

The following information relates to the dataset `restNYC` which is loaded into your workspace:

-   each row represents one customer survey from Italian restaurants in NYC
-   Price = price (in US$) of dinner (including tip and one drink)
-   Service = rating of the service (from 1 to 30)
-   Food = rating of the food (from 1 to 30)
-   Decor = rating of the decor (from 1 to 30)

```r
# Output the first model

lm(Price ~ Service, data = restNYC) %>% tidy()

  

# Output the second model

lm(Price ~ Service + Food + Decor, data = restNYC) %>% tidy()
```

### Interpreting coefficients

What is the correct interpretation of the coefficient on `Service` in the linear model which regresses `Price` on `Service`, `Food`, and `Decor`?

You will need to run the linear model before answering the question: `lm(Price ~ Service + Food + Decor, data=restNYC) %>% tidy()`

  
**Given that `Food` and `Decor` are in the model, `Service` is not significant, and we cannot know whether it has effect on modeling `Price`.**